{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Dataverse team is planning to add  additions to the dataset deposit form in the Harvard Dataverse Repository, we need to know:\n",
    "\n",
    "- In Dataverse repositories, how many datasets have funding metadata?\n",
    "- And among those, how many have funder names?\n",
    "- Which users include funding metadata in their deposits most often?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './metadata/dataset_pids_from_most_known_dataverse_installations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/t7/6b3g2zgx3cz_2rfy3pmdtnch0000gq/T/ipykernel_1297/2745469921.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# they're published in, removing the PIDs of datasets whose metadata could not be\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# saved, i.e. dataverse_json_export_saved is FALSE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m datasetPIDsDF = (pd\n\u001B[0m\u001B[1;32m      5\u001B[0m     .read_csv(\n\u001B[1;32m      6\u001B[0m         \u001B[0;34m'./metadata/dataset_pids_from_most_known_dataverse_installations.csv'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './metadata/dataset_pids_from_most_known_dataverse_installations.csv'"
     ]
    }
   ],
   "source": [
    "# Import CSV file that lists PIDs of all datasets and which Dataverse installation\n",
    "# they're published in, removing the PIDs of datasets whose metadata could not be\n",
    "# saved, i.e. dataverse_json_export_saved is FALSE\n",
    "datasetPIDsDF = (pd\n",
    "    .read_csv(\n",
    "        './metadata/dataset_pids_from_most_known_dataverse_installations.csv',\n",
    "        usecols=lambda x: x not in ['dataset_pid', 'dataverse_name'],\n",
    "        sep=',', na_filter=False)\n",
    "    .query('(dataverse_json_export_saved == True)')\n",
    "    .drop(columns=['dataverse_json_export_saved'])\n",
    "    .reset_index(drop=True, inplace=False)\n",
    ")\n",
    "\n",
    "datasetPIDsDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import basic metadata of all dataset versions\n",
    "datasetVersionMetadataDF = pd.read_csv(\n",
    "    './metadata/basic_metadata_2022.10.02-2022.10.03.csv',\n",
    "    usecols=lambda x: x not in [\n",
    "        'dataset_pid', 'dataset_publication_date',\n",
    "        'dataset_version_state', 'publisher'],\n",
    "    parse_dates=['dataset_version_create_time'],\n",
    "    sep=',', na_filter=False)\n",
    "\n",
    "datasetVersionMetadataDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Count of datasets: {len(datasetPIDsDF)}')\n",
    "print(f'Count of dataset versions: {len(datasetVersionMetadataDF)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# From the datasetVersionMetadataDF, lets use the version publication dates to get the PIDs and dataset version numbers of only the most recently published versions of each dataset. The resulting dataframe should contain the same number of rows as the datasetPIDsDF.\n",
    "\n",
    "latestDatasetVersionsDF = (datasetVersionMetadataDF\n",
    "    .iloc[\n",
    "        datasetVersionMetadataDF\n",
    "        .groupby('dataset_pid_url')['dataset_version_create_time']\n",
    "        .agg(pd.Series.idxmax)]\n",
    "    .reset_index(drop=True, inplace=False))\n",
    "\n",
    "latestDatasetVersionsDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Count of datasets: {len(datasetPIDsDF)}')\n",
    "print(f'Count of rows in latestDatasetVersionsDF: {len(latestDatasetVersionsDF)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join the latestDatasetVersionsDF and the datasetPIDsDF to add the installation column,\n",
    "# so we know which installations published each dataset\n",
    "\n",
    "basicDatasetMetadataDF = (pd\n",
    "    .merge(latestDatasetVersionsDF, datasetPIDsDF,\n",
    "        how='inner',\n",
    "        on=['dataset_pid_url'])\n",
    "    .reset_index(drop=True, inplace=False))\n",
    "\n",
    "# Make sure the count of rows is the same as the count of total datasets: 340,857\n",
    "print(len(basicDatasetMetadataDF))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "basicDatasetMetadataDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# basicDatasetMetadataDF.to_csv('basicDatasetMetadataDF.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import funding metadata, dropping the dataset_pid column\n",
    "grantInformationMetadataDF = pd.read_csv(\n",
    "    './metadata/grant_information(citation)_2022.10.02-2022.10.03.csv',\n",
    "    usecols=lambda x: x not in ['dataset_pid'],\n",
    "    # sep=',', na_filter=False)\n",
    "    sep=',')\n",
    "\n",
    "print(f'Count of rows in grantInformationMetadataDF: {len(grantInformationMetadataDF)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join grantInformationMetadataDF with basicDatasetMetadataDF to retain metadata of\n",
    "# each dataset's latest version\n",
    "\n",
    "grantInformationLatestVersionDF = (pd\n",
    "    .merge(grantInformationMetadataDF, basicDatasetMetadataDF,\n",
    "        how='inner',\n",
    "        on=['dataset_pid_url', 'dataset_version_number'])\n",
    "    .drop(columns=[\n",
    "        'dataset_version_create_time', 'installation',\n",
    "        'dataverse_alias'])\n",
    "    .reset_index(drop=True, inplace=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import contributor metadata, where a funding agency might be listed as a contributor\n",
    "contributorMetadataDF = pd.read_csv(\n",
    "    './metadata/contributor(citation)_2022.10.02-2022.10.03.csv',\n",
    "    usecols=lambda x: x not in ['dataset_pid'],\n",
    "    sep=',', na_filter=False)\n",
    "\n",
    "print(len(contributorMetadataDF))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join contributorMetadataDF with basicDatasetMetadataDF to retain metadata of\n",
    "# each dataset's latest version\n",
    "\n",
    "contributorLatestVersionDF = (pd\n",
    "    .merge(contributorMetadataDF, basicDatasetMetadataDF,\n",
    "        how='inner',\n",
    "        on=['dataset_pid_url', 'dataset_version_number'])\n",
    "    .drop(columns=[\n",
    "        'dataset_version_create_time', 'installation',\n",
    "        'dataverse_alias'])\n",
    "    .reset_index(drop=True, inplace=False))\n",
    "\n",
    "# contributorLatestVersionDF = contributorLatestVersionDF.drop(columns=[\n",
    "#     'dataset_version_create_time', 'installation', 'dataverse_alias'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the columns on all three dataframes basicDatasetMetadataDF, grantInformationLatestVersionDF and contributorLatestVersionDF\n",
    "# All three dataframes should have the 'dataset_pid_url' and 'dataset_version_number' columns\n",
    "\n",
    "print('Columns in basicDatasetMetadataDF:')\n",
    "for i in list(basicDatasetMetadataDF.columns):\n",
    "    print(i)\n",
    "print(f'\\nColumns in grantInformationLatestVersionDF:')\n",
    "for i in list(grantInformationLatestVersionDF.columns):\n",
    "    print(i)\n",
    "print(f'\\nColumns in contributorLatestVersionDF:')\n",
    "for i in list(contributorLatestVersionDF.columns):\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine the basicDatasetMetadataDF, grantInformationLatestVersionDF, and contributorLatestVersionDF with a\n",
    "# full outer join on dataset_pid_url and dataset_version_number columns\n",
    "\n",
    "dataframes = [basicDatasetMetadataDF, grantInformationLatestVersionDF, contributorLatestVersionDF]\n",
    "indexList = ['dataset_pid_url', 'dataset_version_number']\n",
    "for df in dataframes:\n",
    "    df.set_index(indexList, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fundingDatasetMetadataInDataverseInstallationsDF = reduce(lambda left, right: left.join(right, how='outer'), dataframes)\n",
    "fundingDatasetMetadataInDataverseInstallationsDF = fundingDatasetMetadataInDataverseInstallationsDF.reset_index(drop=False, inplace=False)\n",
    "fundingDatasetMetadataInDataverseInstallationsDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, make sure that the number of datasets in the dataframe match the number of total datasets: 340,857\n",
    "countOfDatasetsInfundingDatasetMetadataInDataverseInstallationsDF = len(pd.unique(fundingDatasetMetadataInDataverseInstallationsDF['dataset_pid_url']))\n",
    "print(f'Number of datasets in fundingDatasetMetadataInDataverseInstallationsDF: {countOfDatasetsInfundingDatasetMetadataInDataverseInstallationsDF}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fundingDatasetMetadataInDataverseInstallationsDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fundingDatasetMetadataInDataverseInstallationsDF.to_csv('fundingDatasetMetadataInDataverseInstallationsDF.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring the data\n",
    "\n",
    "Now that we've got the funding metadata of the latest versions of all datasets in the Dataverse installations, let's start answering our questions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collections with most funding metadata and most used funder agency names\n",
    "- In the Harvard Dataverse Repository, which collections have datasets with the most funding metadata? This will help us figure out who to learn from when we make changes to how funding metadata is entered.\n",
    "- Which funder agency names are entered most often? Knowing that might help us figure out how effective our efforts to standardize funder agency name metadata could be? For example, do the changes to the metadata fields (the \"CV javascript\") make it easier for depositors to enter the most popular funder agency names? How much easier?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets start by creating a dataframe containing only metadata of datasets published in the Harvard Dataverse Repository\n",
    "datasetInHDVDF = (fundingDatasetMetadataInDataverseInstallationsDF\n",
    "    .query('(installation == \"Harvard Dataverse\")')\n",
    "    .drop(columns=['installation'])\n",
    "    .reset_index(drop=True, inplace=False)\n",
    "    )\n",
    "\n",
    "datasetInHDVDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countOfDatasetsInHDVDF = len(pd.unique(datasetInHDVDF['dataset_pid_url']))\n",
    "print(f'Number of datasets in datasetInHDVDF: {countOfDatasetsInHDVDF}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now let's keep the metadata of the datasets that have funding metadata in the three fields:\n",
    "# grantNumberAgency, grantNumberValue, and contributorName when contributorType is \"Funder\"\n",
    "fundingDatasetMetadataInHDVDF = (\n",
    "    datasetInHDVDF.query(\n",
    "        '(grantNumberAgency == grantNumberAgency) or\\\n",
    "        (grantNumberValue == grantNumberValue) or\\\n",
    "        (contributorType == \"Funder\" and contributorName == contributorName)')\n",
    "     .reset_index(drop=True, inplace=False)\n",
    "     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fundingDatasetMetadataInHDVDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Number of datasets in fundingDatasetMetadataInHDVDF: {(len(pd.unique(fundingDatasetMetadataInHDVDF[\"dataset_pid_url\"])))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So of the 80,278 datasets in the Harvard Dataverse Repository as of early October 2022, 30,222 had metadata about funding in one of the three fields where we expect it.\n",
    "\n",
    "What's entered most often in the grantNumberAgency (Funding Information Name) field?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new dataframe that lists each funder name entered in grantNumberAgency and the number of datasets with that funder name\n",
    "datasetCountByFundingAgencyNamesInHDV = (\n",
    "    fundingDatasetMetadataInHDVDF[['dataset_pid_url', 'grantNumberAgency']]\n",
    "        .query('grantNumberAgency == grantNumberAgency')\n",
    "        .drop_duplicates()\n",
    "        .groupby(['grantNumberAgency']).count()\n",
    "        .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "        .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "        .reset_index(drop=False, inplace=False)\n",
    ")\n",
    "\n",
    "datasetCountByFundingAgencyNamesInHDV.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's most likely that the Leon Levy collection has the most datasets with a funder name in their grantNumberAgency (Funding Information Name) field.\n",
    "\n",
    "It also looks like, for datasets with other funder name metadata, we might have to deal with the variations in spellings, maybe using a deduplication or fuzzy matching library, in order to see which funders are added to the funding metadata most often. For example, this shows only 57 datasets with the value \"NIH\" in the Funding Information Agency field, but I've found more than 57 datasets created from NIH-funded research, with other values in the field, such as \"National Institute of Health\".\n",
    "\n",
    "For now, let's see which collections have the most datasets with funding metadata, excluding the Leon Levy collections."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetsInHDVByCollection_NoLevy = (\n",
    "    fundingDatasetMetadataInHDVDF[['dataset_pid_url', 'dataverse_alias']]\n",
    "    .drop_duplicates()\n",
    "    .groupby(['dataverse_alias']).count()\n",
    "    .query('~dataverse_alias.str.contains(\"levy_photos\").values')\n",
    "    .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "    .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "    .reset_index(drop=False, inplace=False)\n",
    ")\n",
    "\n",
    "datasetsInHDVByCollection_NoLevy.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The IFPRI collections (IFPRI and AfricaRISING), worldfish collection and CIAT collection have the most datasets with funding metadata. (\"harvard\" is the alias of repository's main collection, where anyone can add data.)\n",
    "\n",
    "What have the depositors of those datasets entered in the metadata?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fundingDatasetMetadataInHDVDF.to_csv('fundingDatasetMetadataInHDVDF.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def funder_names_in_collections(dataframe, collectionAliasesList):\n",
    "    funderNamesInCollectionDF = (\n",
    "        # Drop all but the needed columns\n",
    "        dataframe[[\n",
    "            'dataset_pid_url', 'grantNumberAgency', 'dataverse_alias']]\n",
    "        # Remove all datasets except those in given list of collection aliases\n",
    "        .query(\n",
    "            'dataverse_alias in @collectionAliasesList')\n",
    "        # Remove the 'dataverse_alias' column\n",
    "        .drop(columns=['dataverse_alias'])\n",
    "        # Keep only datasets that have funding agency name metadata\n",
    "        .query('grantNumberAgency == grantNumberAgency')\n",
    "        # Drop duplicate rows\n",
    "        .drop_duplicates()\n",
    "        # Group by funding agency name\n",
    "        .groupby(['grantNumberAgency']).count()\n",
    "        .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "        # Sort by count of occurrences of each funder name\n",
    "        .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "        .reset_index(drop=False, inplace=False))\n",
    "    return funderNamesInCollectionDF\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetCountByFundingAgencyNamesIFPRI = funder_names_in_collections(\n",
    "    dataframe=fundingDatasetMetadataInHDVDF,\n",
    "    collectionAliasesList=['IFPRI', 'AfricaRISING'])\n",
    "\n",
    "datasetCountByFundingAgencyNamesIFPRI.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetCountByFundingAgencyNamesWorldfish = funder_names_in_collections(\n",
    "    dataframe=fundingDatasetMetadataInHDVDF,\n",
    "    collectionAliasesList=['worldfish'])\n",
    "\n",
    "datasetCountByFundingAgencyNamesWorldfish.head()\n",
    "\n",
    "# funderNamesList_Worldfish = list(set(datasetCountByFundingAgencyNamesWorldfish[\"grantNumberAgency\"].values.tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetCountByFundingAgencyNamesCIAT = funder_names_in_collections(\n",
    "    dataframe=fundingDatasetMetadataInHDVDF,\n",
    "    collectionAliasesList=[ # Aliases of collections I think are associated with CIAT\n",
    "        'CIAT',\n",
    "        'AgBio',\n",
    "        'AICCRA',\n",
    "        'CIFOR',\n",
    "        'gender',\n",
    "        'crp6',\n",
    "        'dapa',\n",
    "        'AllianceBioversityCIATFoodConsumer',\n",
    "        'AllianceBioversityCIATLandscapes',\n",
    "        'AllianceBioversityCIATClimate',\n",
    "        'AllianceBioversityCIATBiodiversity',\n",
    "        'AllianceBioversityCIATDigital',\n",
    "        'AllianceBioversityCIATCrops4NH',\n",
    "        'soils',\n",
    "        'AllianceBioversityCIAT'\n",
    "    ])\n",
    "\n",
    "datasetCountByFundingAgencyNamesCIAT.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetCountByFundingAgencyNamesDFEEP = funder_names_in_collections(\n",
    "    dataframe=fundingDatasetMetadataInHDVDF,\n",
    "    collectionAliasesList=['DFEEP', 'ipa', 'jpal'])\n",
    "\n",
    "datasetCountByFundingAgencyNamesDFEEP.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicate funding metadata in Harvard Dataverse\n",
    "\n",
    "There are two fields where depositors can enter the names of funders: In the Funding Information Agency field and in the Contributor field when they choose the Contributor Type \"Funder\". How often has this happened and who's done it? By learning these things, we can see how big the problem is and who's used both fields? And later we can ask those people why. We need to learn if the design of the fields are meeting some need that we weren't aware of.\n",
    "\n",
    "For now, let's continue looking only at the latest version of each dataset. This might cause a further under count of the number of times this issue has actually occurred. For example, first version of a dataset might have both fields filled but the latest might have only one. By considering only the latest version of each dataset, we'll miss cases like this. Eventually we'll have to think about what to do about the metadata of previous dataset versions.\n",
    "\n",
    "Questions\n",
    "- In the Harvard Dataverse Repository, how many datasets have values in the Funder Information fields and in the Contributor field when Contributor Type is \"Funder\"?\n",
    "- How many datasets have funding metadata in their Contributor field and not in their Funder Information fields?\n",
    "- How often are the same values in both fields? For example, one dataset might have \"NIH\" in the Funder Information Agency field and in the Contributor Name field when the Contributor Type is \"Funder\". How often do things like this happen?\n",
    "- How often are different values in both fields?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's remind ourselves what information we have in the fundingDatasetMetadataInHDVDF dataframe\n",
    "fundingDatasetMetadataInHDVDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now let's query it to get only datasets that have values in both metadata fields\n",
    "duplicateFundingFieldsInHDV = (fundingDatasetMetadataInHDVDF\n",
    "        .query(\n",
    "            'grantNumberAgency == grantNumberAgency and\\\n",
    "             (contributorType == \"Funder\" and contributorName == contributorName)')\n",
    "        .sort_values(by=['dataset_pid_url'], ascending=True)\n",
    "        .reset_index(drop=True, inplace=False))\n",
    "\n",
    "duplicateFundingFieldsInHDV.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasetCountDuplicateFundingFields = len(pd.unique(duplicateFundingFieldsInHDV['dataset_pid_url']))\n",
    "print(f'Number of datasets with metadata in both funding metadata fields: {datasetCountDuplicateFundingFields}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# duplicateFundingFieldsInHDV.to_csv('duplicateFundingFieldsInHDV.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's see which collections have most of these datasets\n",
    "countOfDuplicateFundingFieldsInHDVByCollection = (\n",
    "    # Drop all but the needed columns\n",
    "    duplicateFundingFieldsInHDV[[\n",
    "        'dataset_pid_url', 'dataverse_alias']]\n",
    "        # Drop duplicate rows\n",
    "        .drop_duplicates()\n",
    "        # Group by dataverse_alias\n",
    "        .groupby(['dataverse_alias']).count()\n",
    "        .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "        # Sort by count of occurrences of each funder name\n",
    "        .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "        .reset_index(drop=False, inplace=False))\n",
    "\n",
    "countOfDuplicateFundingFieldsInHDVByCollection.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How many datasets have funding metadata in their Contributor field and not in their Grant Information fields?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contributorsButNoFundingInfomration = (fundingDatasetMetadataInHDVDF\n",
    "    .query(\n",
    "        '(contributorType == \"Funder\" and contributorName == contributorName)and\\\n",
    "        grantNumberAgency != grantNumberAgency')\n",
    "    .reset_index(drop=True, inplace=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contributorsButNoFundingInfomration.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# contributorsButNoFundingInfomration.to_csv('contributorsButNoFundingInfomration.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicate funder metadata in other Dataverse installations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fundingDatasetMetadataInDataverseInstallationsDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fundingDatasetMetadataInDataverseInstallationsDF.to_csv('fundingDatasetMetadataInDataverseInstallationsDF.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "countOfDatasetsInEachInstallationDF = (\n",
    "    # Drop all but the needed columns\n",
    "    fundingDatasetMetadataInDataverseInstallationsDF[[\n",
    "        'dataset_pid_url', 'installation']]\n",
    "        # Drop duplicate rows\n",
    "        .drop_duplicates()\n",
    "        # Group by dataverse_alias\n",
    "        .groupby(['installation']).count()\n",
    "        .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "        # Sort by count of occurrences of each funder name\n",
    "        .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "        .reset_index(drop=False, inplace=False))\n",
    "\n",
    "countOfDatasetsInEachInstallationDF.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets see how many of these datasets in each installation have values in both fields\n",
    "duplicateFundingFieldsInAllInstallations = (fundingDatasetMetadataInDataverseInstallationsDF\n",
    "    .query(\n",
    "        'grantNumberAgency == grantNumberAgency and\\\n",
    "         (contributorType == \"Funder\" and contributorName == contributorName)')\n",
    "    .sort_values(by=['dataset_pid_url'], ascending=True)\n",
    "    .reset_index(drop=True, inplace=False))\n",
    "\n",
    "duplicateFundingFieldsInAllInstallations.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# duplicateFundingFieldsInAllInstallations.to_csv('duplicateFundingFieldsInAllInstallations.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'duplicateFundingFieldsInAllInstallations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/t7/6b3g2zgx3cz_2rfy3pmdtnch0000gq/T/ipykernel_1297/633098253.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m countOfDatasetsWithDuplicateFundingFieldsInEachInstallationDF = (\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# Drop all but the needed columns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     duplicateFundingFieldsInAllInstallations[[\n\u001B[0m\u001B[1;32m      4\u001B[0m         'dataset_pid_url', 'installation']]\n\u001B[1;32m      5\u001B[0m         \u001B[0;31m# Drop duplicate rows\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'duplicateFundingFieldsInAllInstallations' is not defined"
     ]
    }
   ],
   "source": [
    "countOfDatasetsWithDuplicateFundingFieldsInEachInstallationDF = (\n",
    "    # Drop all but the needed columns\n",
    "    duplicateFundingFieldsInAllInstallations[[\n",
    "        'dataset_pid_url', 'installation']]\n",
    "        # Drop duplicate rows\n",
    "        .drop_duplicates()\n",
    "        # Group by dataverse_alias\n",
    "        .groupby(['installation']).count()\n",
    "        .rename(columns={'dataset_pid_url': 'count_of_datasets'})\n",
    "        # Sort by count of occurrences of each funder name\n",
    "        .sort_values(by=['count_of_datasets'], ascending=False)\n",
    "        .reset_index(drop=False, inplace=False))\n",
    "\n",
    "countOfDatasetsWithDuplicateFundingFieldsInEachInstallationDF.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}