{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from google_trans_new import google_translator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "detector = google_translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Harvard Dataverse Repository is working on a CoreTrustSeal application and is considering adding the number of languages used in the metadata of published datasets.\n",
    "\n",
    "In this notebook we'll be detecting the languages of text entered in the description metadata field of published datasets in the Harvard Dataverse Repository, since it's a required field and it's the required field most likely to contain enough text to determine a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the description metadata of datasets in the Harvard Dataverse Repository, I'm importing a CSV file that I created using a series of Python scripts and the Dataverse APIs for collecting metadata of known Dataverse repositories. The metadata for each known repository is in a dataset published at https://doi.org/10.7910/DVN/DCDKZQ. The scripts used to convert the metadata, expressed as JSON documents, into CSV files is at https://github.com/jggautier/dataverse_scripts/tree/master/get-dataverse-metadata/parse_metadata_fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptionMetadata = pd.read_csv('citation_dsDescription.csv', na_filter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataframe \"descriptionMetadata\" contains the text entered in a description field in the latest published version of every dataset in the repository. We want to figure out which language those entries were written in, then get a count of the unique languages to include in the CoreTrustSeal application.\n",
    "\n",
    "To detect languages of the text, we'll be using a Python library called google_trans_new (https://github.com/lushan88a/google_trans_new), which uses the Google Translate API. (Google has it's own Python library but there are limitation and unresolved bugs that have made using the library difficult.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'indonesian']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detect_result_2 = detector.detect('tanpa izin dari distributor data sebelumnya')\n",
    "print(detect_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
