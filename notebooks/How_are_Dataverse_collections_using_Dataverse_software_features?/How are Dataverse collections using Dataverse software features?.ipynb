{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-modules-and-load-functions\" data-toc-modified-id=\"Import-modules-and-load-functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import modules and load functions</a></span></li><li><span><a href=\"#Load-misc.-functions\" data-toc-modified-id=\"Load-misc.-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load misc. functions</a></span></li><li><span><a href=\"#Get-repository-and-dataverse-info\" data-toc-modified-id=\"Get-repository-and-dataverse-info-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get repository and dataverse info</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-IDs-of-any-sub-dataverses-in-the-given-dataverse\" data-toc-modified-id=\"Get-IDs-of-any-sub-dataverses-in-the-given-dataverse-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Get IDs of any sub-dataverses in the given dataverse</a></span></li></ul></li><li><span><a href=\"#Get-updated-dataset-info\" data-toc-modified-id=\"Get-updated-dataset-info-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Get updated dataset info</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-dataset-IDs-and-(sub)dataverse-names\" data-toc-modified-id=\"Get-dataset-IDs-and-(sub)dataverse-names-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Get dataset IDs and (sub)dataverse names</a></span></li><li><span><a href=\"#Get-updated-info-about-dataverse's-dataset-and-files\" data-toc-modified-id=\"Get-updated-info-about-dataverse's-dataset-and-files-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Get updated info about dataverse's dataset and files</a></span></li><li><span><a href=\"#Join-datasetPIDs-and-datasetInfo-dataframes\" data-toc-modified-id=\"Join-datasetPIDs-and-datasetInfo-dataframes-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Join datasetPIDs and datasetInfo dataframes</a></span></li><li><span><a href=\"#Export-report-to-CSV\" data-toc-modified-id=\"Export-report-to-CSV-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Export report to CSV</a></span></li></ul></li><li><span><a href=\"#Get-existing-dataset-info-of-dataverse\" data-toc-modified-id=\"Get-existing-dataset-info-of-dataverse-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Get existing dataset info of dataverse</a></span></li><li><span><a href=\"#Create-summary-stats-of-dataverse\" data-toc-modified-id=\"Create-summary-stats-of-dataverse-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create summary stats of dataverse</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from functools import reduce\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load misc. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def list_to_string(list):\n",
    "    # Alphabetize list in case-insensitive way\n",
    "    list = sorted(list, key=lambda s: s.casefold())\n",
    "\n",
    "    # Change list to comma-separated string\n",
    "    delimiter = \",\"\n",
    "    string = delimiter.join(list)\n",
    "    return string\n",
    "\n",
    "\n",
    "def string_to_list(string): \n",
    "    li = list(string.split(\",\")) \n",
    "    return li\n",
    "\n",
    "\n",
    "def string_to_datetime(string):\n",
    "    dateTime = datetime.strptime(string, '%Y-%m-%dT%H:%M:%S%z')\n",
    "    return dateTime\n",
    "\n",
    "\n",
    "def make_percent(number):\n",
    "    return(str(round(number * 100, 2)) + '%')\n",
    "\n",
    "\n",
    "currentTimeUTC = datetime.now(timezone.utc)\n",
    "currentTimeLocal = time.strftime('%Y.%m.%d_%H.%M.%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get repository and Dataverse collection info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataverse server and alias from user.\n",
    "mainDataverseUrl = 'https://dataverse.harvard.edu/dataverse/ifpri'\n",
    "\n",
    "parsed = urlparse(mainDataverseUrl)\n",
    "server = parsed.scheme + '://' + parsed.netloc\n",
    "try:\n",
    "    mainDataverseAlias = parsed.path.split('/')[2]\n",
    "except IndexError:\n",
    "    mainDataverseAlias = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository_metadatablocks(server):\n",
    "    repositoryMetadataBlocksApi = '%s/api/v1/metadatablocks' % (server)\n",
    "    response = requests.get(repositoryMetadataBlocksApi)\n",
    "    repositoryMetadataBlocks = response.json()\n",
    "\n",
    "    repositoryMetadataBlockNames = []\n",
    "    for repositoryMetadataBlock in repositoryMetadataBlocks['data']:\n",
    "        repositoryMetadataBlockNames.append(repositoryMetadataBlock['name'])\n",
    "    return repositoryMetadataBlockNames\n",
    "\n",
    "repositoryMetadataBlockNames = get_repository_metadatablocks(server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get metadata about dataverse\n",
    "def get_main_dataverse_json(mainDataverseUrl):\n",
    "    dataverseInfoApi = '%s/api/dataverses/%s' % (server, mainDataverseAlias)\n",
    "    response = requests.get(dataverseInfoApi)\n",
    "    dataverseMetadata = response.json()\n",
    "    return dataverseMetadata\n",
    "\n",
    "\n",
    "dataverseMetadata = get_main_dataverse_json(mainDataverseUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if dataverseMetadata['status'] == 'ERROR' or mainDataverseAlias == '' or dataverseMetadata['data']['id'] == 1:\n",
    "    print('No dataverse found or dataverse is the root dataverse.\\\n",
    " Please make sure the dataverse is published and not the repository\\'s root dataverse.')\n",
    "\n",
    "if dataverseMetadata['status'] == 'OK':\n",
    "    def dataverse_description_exists():\n",
    "        if 'description' in dataverseMetadata['data']:\n",
    "            dataverseDescriptionExists = True\n",
    "        else:\n",
    "            dataverseDescriptionExists = False\n",
    "        return dataverseDescriptionExists\n",
    "\n",
    "\n",
    "    def dataverse_tagline_exists():\n",
    "        if 'theme' in dataverseMetadata['data'] and 'tagline' in dataverseMetadata['data']['theme']:\n",
    "            taglineExists = True\n",
    "        else:\n",
    "            taglineExists = False\n",
    "        return taglineExists\n",
    "\n",
    "\n",
    "    def dataverse_facets():\n",
    "        dataverseFacetsApi = '%s/api/dataverses/%s/facets' % (server, mainDataverseAlias)\n",
    "        response = requests.get(dataverseFacetsApi)\n",
    "        dataverseFacets = response.json()\n",
    "        facets = []\n",
    "        for facet in dataverseFacets['data']:\n",
    "            facets.append(facet)\n",
    "        return facets\n",
    "\n",
    "\n",
    "    def dataverse_metadatablocks():\n",
    "        dataverseMetadatablocksApi = '%s/api/dataverses/%s/metadatablocks' % (server, mainDataverseAlias)\n",
    "        response = requests.get(dataverseMetadatablocksApi)\n",
    "        dataverseMetadatablocks = response.json()\n",
    "        dataverseMetadatablocksList = []\n",
    "        for dataverseMetadatablock in dataverseMetadatablocks['data']:\n",
    "            dataverseMetadatablocksList.append(dataverseMetadatablock['name'])\n",
    "        return dataverseMetadatablocksList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IDs of any Dataverse collections in the given Dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 dataverse and 0 subdataverses\n"
     ]
    }
   ],
   "source": [
    "def dataverse_ids():\n",
    "    mainDataverseID = dataverseMetadata['data']['id']\n",
    "    dataverseIDs = [mainDataverseID]\n",
    "    for dataverseID in dataverseIDs:\n",
    "\n",
    "        getContentsApi = '%s/api/dataverses/%s/contents' % (server, dataverseID)\n",
    "\n",
    "        response = requests.get(getContentsApi)\n",
    "        dataverseContents = response.json()\n",
    "\n",
    "        for i in dataverseContents['data']:\n",
    "            if i['type'] == 'dataverse':\n",
    "                dataverseID = i['id']\n",
    "                dataverseIDs.extend([dataverseID])\n",
    "    return dataverseIDs\n",
    "\n",
    "print('\\nFound 1 dataverse and %s subdataverses' % (len(dataverse_ids()) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get updated dataset info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset IDs and (sub)dataverse names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dataframe with PIDs of all published datasets and their owning (sub)dataverse\n",
    "def get_datasetPIDs():\n",
    "    datasetPIDs = []\n",
    "    datasetInfoDict = []\n",
    "    for dataverseID in dataverse_ids():\n",
    "        getDataverseInfoApi = '%s/api/dataverses/%s' % (server, dataverseID)\n",
    "        response = requests.get(getDataverseInfoApi)\n",
    "        dataverseInfo = response.json()\n",
    "        dataverseName = dataverseInfo['data']['name']\n",
    "        dataverseAlias = dataverseInfo['data']['alias']\n",
    "\n",
    "        getDataverseContentsApi = '%s/api/dataverses/%s/contents' % (server, dataverseID)\n",
    "        response = requests.get(getDataverseContentsApi)\n",
    "        dataverseContents = response.json()\n",
    "        for item in dataverseContents['data']:\n",
    "            if item['type'] == 'dataset':\n",
    "                datasetPID = item['persistentUrl'].replace('https://doi.org/', 'doi:')\n",
    "                datasetPIDs.append(datasetPID)\n",
    "\n",
    "                newRow = {'datasetPID': datasetPID,\n",
    "                      'dataverseName': dataverseName,\n",
    "                      'dataverseUrl': '%s/dataverse/%s' % (server, dataverseAlias)\n",
    "                     }\n",
    "                datasetInfoDict.append(dict(newRow))\n",
    "    datasetDataverseInfoDF = pd.DataFrame(datasetInfoDict)\n",
    "    return datasetDataverseInfoDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "402"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetPIDsCount = len(get_datasetPIDs().index)\n",
    "datasetPIDsCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get updated info about Dataverse collection's datasets and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/t7/6b3g2zgx3cz_2rfy3pmdtnch0000gq/T/ipykernel_7570/552730580.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    175\u001B[0m                   \u001B[0;34m'restrictedFilesCount'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrestrictedFilesCount\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m                  }\n\u001B[0;32m--> 177\u001B[0;31m         \u001B[0mdatasetInfoDict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewRow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0mdatasetCount\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'%s of %s (%s)'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdatasetCount\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatasetPIDsCount\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatasetPID\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'\\r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflush\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Getting this info can be slow. For example, getting the info of ~375 datasets might take about 45 min\n",
    "\n",
    "# Create list of file types that Dataverse can convert to .tab files during ingest\n",
    "ingestableFileTypes = ['application/x-rlang-transport', 'application/x-stata-13', 'application/x-spss-por',\n",
    "                      'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'text/csv', 'text/tsv',\n",
    "                      'application/x-spss-sav', 'text/comma-separated-values', 'application/x-stata',\n",
    "                      'application/x-stata-14']\n",
    "\n",
    "datasetInfoList = []\n",
    "datasetCount = 0\n",
    "unretrievableMetadata = []\n",
    "for datasetPID in get_datasetPIDs()['datasetPID']:\n",
    "    getAllVersionsApi = '%s/api/datasets/:persistentId/versions?persistentId=%s' % (server, datasetPID)\n",
    "    response = requests.get(getAllVersionsApi)\n",
    "    datasetVersions = response.json()\n",
    "    \n",
    "#     print('Getting info about %s' % (datasetPID), end='\\r', flush=True)\n",
    "    \n",
    "    # Get only datasets with metadata (exclude responses with no values in 'data' key, e.g. deaccessioned datasets)\n",
    "    if datasetVersions['status'] == 'OK' and len(datasetVersions['data']) > 0:\n",
    "        \n",
    "        # Get metadata of latest version\n",
    "        latestDatasetVersion = datasetVersions['data'][0]\n",
    "        \n",
    "        # Get index location of first dataset version\n",
    "        firstVersion = len(datasetVersions['data']) - 1\n",
    "\n",
    "        publicationDate = string_to_datetime(datasetVersions['data'][firstVersion]['releaseTime'])\n",
    "        latestReleaseDate = string_to_datetime(latestDatasetVersion['releaseTime'])\n",
    "        \n",
    "        # Get age of dataset from today's date\n",
    "        delta = currentTimeUTC - publicationDate\n",
    "        ageOfDataset = delta.days\n",
    "        \n",
    "        # Get number of days since last update\n",
    "        delta = currentTimeUTC - latestReleaseDate\n",
    "        ageOfLastUpdate = delta.days\n",
    "        if ageOfLastUpdate < 0:\n",
    "            ageOfLastUpdate = 0\n",
    "        \n",
    "        # Get length of description text\n",
    "        descriptionLength = 0\n",
    "        \n",
    "        for field in latestDatasetVersion['metadataBlocks']['citation']['fields']:\n",
    "            if field['typeName'] == 'dsDescription':\n",
    "                # \"N/A\" is the value assigned when there was no description given (pre Dataverse 4)\n",
    "                if len(field['value']) == 1 and field['value'][0]['dsDescriptionValue']['value'] == 'N/A':\n",
    "                    descriptionLength = 0\n",
    "                else:\n",
    "                    for i in field['value']:\n",
    "                        descriptionLength = descriptionLength + len(i['dsDescriptionValue']['value'])\n",
    "\n",
    "        # See whether CC0, Terms of Use, or Terms of Access metadata exists\n",
    "        license = latestDatasetVersion.get('license', 'None')\n",
    "\n",
    "        if 'termsOfUse' in latestDatasetVersion:\n",
    "            termsOfUse = True\n",
    "        else:\n",
    "            termsOfUse = False\n",
    "            \n",
    "        if 'termsOfAccess' in latestDatasetVersion:\n",
    "            termsOfAccess = True\n",
    "        else:\n",
    "            termsOfAccess = False\n",
    "\n",
    "        if license != 'CC0' and termsOfUse == False and termsOfAccess == False:\n",
    "            termsExist = False\n",
    "        else:\n",
    "            termsExist = True\n",
    "\n",
    "        # Get info about related publication metadata\n",
    "        relPubCount = 0\n",
    "        relPubPIDCount = 0\n",
    "        for field in latestDatasetVersion['metadataBlocks']['citation']['fields']:\n",
    "            if field['typeName'] == 'publication':\n",
    "                for value in field['value']:\n",
    "                    relPubCount += 1\n",
    "                    if 'publicationIDType' and 'publicationIDNumber' in value:\n",
    "                        relPubPIDCount += 1\n",
    "        \n",
    "        # Show metadatablocks whose fields are used by the dataset\n",
    "        usedMetadataBlocks = []\n",
    "        for repositoryMetadataBlockName in repositoryMetadataBlockNames:\n",
    "            try:\n",
    "                fieldCount = len(latestDatasetVersion['metadataBlocks'][repositoryMetadataBlockName]['fields'])\n",
    "                if fieldCount > 0:\n",
    "                    usedMetadataBlocks.append(repositoryMetadataBlockName)\n",
    "            except KeyError:\n",
    "                usedMetadataBlocks = usedMetadataBlocks\n",
    "        if len(usedMetadataBlocks) == 0:\n",
    "            usedMetadataBlocks = ''\n",
    "        else:\n",
    "            usedMetadataBlocks = list_to_string(usedMetadataBlocks)\n",
    "        \n",
    "        # Get number of files\n",
    "        numberOfFiles = len(latestDatasetVersion['files'])\n",
    "\n",
    "        # Get file info\n",
    "        noFileDescriptionCount = 0\n",
    "        contentTypes = []\n",
    "        ingestedTabFilesCount = 0\n",
    "        uningestedTabFilesCount = 0\n",
    "        restrictedFilesCount = 0\n",
    "        fileTags = []\n",
    "        for file in latestDatasetVersion['files']:            \n",
    "            if 'description' in file:\n",
    "                noFileDescriptionCount = noFileDescriptionCount\n",
    "            else:\n",
    "                noFileDescriptionCount += 1\n",
    "\n",
    "            contentTypes.append(file['dataFile']['contentType'])\n",
    "\n",
    "            if file['restricted'] == True:\n",
    "                restrictedFilesCount += 1\n",
    "\n",
    "            contentType = file['dataFile']['contentType']\n",
    "            fileSize = file['dataFile']['filesize']\n",
    "            fileName = file['dataFile']['filename'].lower()\n",
    "            if contentType in ingestableFileTypes:\n",
    "                # If it's an RData file that's 1MB or smaller, count it as an uningested file that may be ingested\n",
    "                if contentType == 'application/x-rlang-transport' and fileSize < 1000001:\n",
    "                    uningestedTabFilesCount += 1\n",
    "                # If it's another ingestable file type that's 150MB or smaller, count it as an uningested file that may be ingested\n",
    "                elif fileSize < 150000001:\n",
    "                    uningestedTabFilesCount += 1\n",
    "            # For some CSV files, Harvard Dataverse repository records a generic contentType, so we need to look at the file name extension instead\n",
    "            if '.csv' in fileName and fileSize < 150000001:\n",
    "                uningestedTabFilesCount += 1\n",
    "            if contentType == 'text/tab-separated-values':\n",
    "                ingestedTabFilesCount += 1\n",
    "\n",
    "            try:\n",
    "                for tags in file['categories']:\n",
    "                    fileTags.append(tags)\n",
    "            except KeyError:\n",
    "                fileTags = fileTags\n",
    "\n",
    "        ingestableTabularFileCount = uningestedTabFilesCount + ingestedTabFilesCount\n",
    "\n",
    "        if len(fileTags) == 0:\n",
    "            fileTagsExist = False\n",
    "        else:\n",
    "            fileTagsExist = True\n",
    "\n",
    "        if len(contentTypes) == 0:\n",
    "            uniqueContentTypes = 'NA'\n",
    "        else:\n",
    "            uniqueContentTypes = list_to_string(list(set(contentTypes)))\n",
    "\n",
    "        # Create dictionary\n",
    "        newRow = {'datasetPID': datasetPID,\n",
    "                  'datasetPIDUrl' : datasetPID.replace('doi:', 'https://doi.org/'),\n",
    "                  'numberOfVersions': len(datasetVersions['data']),\n",
    "                  'numberOfMajorVersions': latestDatasetVersion['versionNumber'],\n",
    "                  'publicationDate': publicationDate,\n",
    "                  'latestReleaseDate': latestReleaseDate,\n",
    "                  'ageOfDataset(Days)': ageOfDataset,\n",
    "                  'ageOfLastUpdate(Days)': ageOfLastUpdate,\n",
    "                  'descriptionLenth': descriptionLength,\n",
    "                  'termsExist': termsExist,\n",
    "                  'license': license,\n",
    "                  'termsOfUseExists': termsOfUse,\n",
    "                  'termsOfAccessExists': termsOfAccess,\n",
    "                  'relPubCount': relPubCount,\n",
    "                  'relPubPIDCount': relPubPIDCount,\n",
    "                  'usedMetadataBlocks': usedMetadataBlocks,\n",
    "                  'numberOfFiles': numberOfFiles,\n",
    "                  'noFileDescriptionCount': noFileDescriptionCount,\n",
    "                  'fileTagsExist': fileTagsExist,\n",
    "                  'uniqueContentTypes': uniqueContentTypes,\n",
    "                  'ingestableTabularFileCount': ingestableTabularFileCount,\n",
    "                  'ingestedTabFilesCount': ingestedTabFilesCount,\n",
    "                  'uningestedTabFilesCount': uningestedTabFilesCount,\n",
    "                  'restrictedFilesCount': restrictedFilesCount\n",
    "                 }\n",
    "        datasetInfoDict.append(dict(newRow))\n",
    "        datasetCount += 1\n",
    "        print('%s of %s (%s)' % (datasetCount, datasetPIDsCount, datasetPID), end='\\r', flush=True)\n",
    "    else:\n",
    "        unretrievableMetadata.append(datasetPID)\n",
    "print('Retrieved dataset info: %s' %(datasetCount))\n",
    "if unretrievableMetadata:\n",
    "    print('Unretrievable dataset info: %s' % (len(unretrievableMetadata)))\n",
    "    print(unretrievableMetadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetInfoDF = pd.DataFrame(datasetInfoDict)\n",
    "datasetInfoDF.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join datasetPIDs and datasetInfo dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [get_datasetPIDs(), datasetInfoDF]\n",
    "\n",
    "# For each dataframe, set the indexes (or the common columns across the dataframes to join on)\n",
    "for dataframe in dataframes:\n",
    "    dataframe.set_index(['datasetPID'], inplace=True)\n",
    "\n",
    "# Merge both dataframes and save new dataframe to \"report\" variable\n",
    "report = reduce(lambda left, right: left.join(right, how='outer'), dataframes)\n",
    "\n",
    "# Remove any rows with missing info (likely due to published datasets where all versions are deaccessioned)\n",
    "report = report.dropna()\n",
    "\n",
    "# Reset index\n",
    "report.reset_index(drop=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export report to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export report to CSV\n",
    "file = '%s_%s.csv' % (mainDataverseAlias, currentTimeLocal)\n",
    "report.to_csv(file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary report of Dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of datasets in the report\n",
    "datasetCount = len(report.index)\n",
    "\n",
    "\n",
    "# Get list of metadatablocks used by all datasets\n",
    "def get_allUsedMetadataBlocks(report):\n",
    "    allUsedMetadataBlocks = []\n",
    "    for i in report['usedMetadataBlocks']:\n",
    "        allUsedMetadataBlocks.extend(list(i.split(\",\")))\n",
    "\n",
    "    # Deduplicate, alphabetize and change list to string\n",
    "    allUsedMetadataBlocks = list_to_string(list(set(allUsedMetadataBlocks)))\n",
    "    return allUsedMetadataBlocks\n",
    "\n",
    "\n",
    "# Get list of uniqueContentTypes used by all datasets\n",
    "def get_allContentTypes(report):\n",
    "    allContentTypes = []\n",
    "    for i in report['uniqueContentTypes']:\n",
    "        if i != 'NA':\n",
    "            allContentTypes.extend(list(i.split(\",\")))\n",
    "    return allContentTypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summaryDict = {\n",
    "    'Summary': mainDataverseAlias,\n",
    "    'Has description': dataverse_description_exists(),\n",
    "    'Has tagline': dataverse_tagline_exists(),\n",
    "    'Number of search facets': len(dataverse_facets()),\n",
    "    'Metadatablocks enabled': len(dataverse_metadatablocks()) - 1,\n",
    "    'Dataset count': datasetCount,\n",
    "    'Versions (avg # of major and minor versions)': round(report['numberOfVersions'].mean(), 2),\n",
    "    'Major versions (average #)': round(report['numberOfMajorVersions'].mean(), 2),\n",
    "    'Age of datasets (average)': round(report['ageOfDataset(Days)'].mean(), 2),\n",
    "    'Dataset description length (avg # of characters)': round(report['descriptionLenth'].mean(), 2),\n",
    "    'CC0 datasets (% of total datasets)': make_percent(len(report[(report['license'] == 'CC0')]) / datasetCount),\n",
    "    'No terms (% of datasets with no terms metadata)': make_percent(((~report['termsExist']).values.sum()) / datasetCount),\n",
    "    'Related pub metadata (% of datasets with rel pub metadata)': make_percent(len(report[(report['relPubCount'] != 0)]) / datasetCount),\n",
    "    'Related pub PIDs (% of datasets with rel pub PIDs)': make_percent(len(report[(report['relPubPIDCount'] != 0)]) / datasetCount),\n",
    "    'Metadatablocks used (list)': get_allUsedMetadataBlocks(report),\n",
    "    'No files (# of datasets with no files)': len(report[(report['numberOfFiles'] == 0)]),\n",
    "    'File descriptions (% of datasets with 1 or more file descriptions)': make_percent(len(report[(report['noFileDescriptionCount'] != 0)]) / datasetCount),\n",
    "    'File tags (% of datasets with 1 or more file tags)': make_percent(((report['fileTagsExist']).values.sum()) / datasetCount),\n",
    "    'Unique file types (count)': len(set(get_allContentTypes(report))),\n",
    "    'Ingestable tabular data (% of datasets with data files that can be ingested)': make_percent(((len(report[(report['ingestedTabFilesCount'] != 0)])) + (len(report[(report['uningestedTabFilesCount'] != 0)]))) / len(report[(report['numberOfFiles'] != 0)])),\n",
    "    'Tabular data ingest successes (% of datasets with tabular data that have been ingested)': make_percent(len(report[(report['ingestedTabFilesCount'] != 0)]) / ((len(report[(report['ingestedTabFilesCount'] != 0)])) + (len(report[(report['uningestedTabFilesCount'] != 0)])))),\n",
    "    'Public files (% of unrestricted files)': make_percent(((report['numberOfFiles'].sum() - report['restrictedFilesCount'].sum()) / report['numberOfFiles'].sum()))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDF = pd.DataFrame.from_records([summaryDict])\n",
    "summaryDF = summaryDF.set_index('Summary').transpose()\n",
    "summaryDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "file = '%s_summary_%s.csv' % (mainDataverseAlias, currentTimeLocal)\n",
    "summaryDF.to_csv(file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}